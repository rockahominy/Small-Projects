{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_Regression",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvK+cbTeMtY2FO9ZQcr3f4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rockahominy/Small-Projects/blob/main/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W5adiltuuvA"
      },
      "source": [
        "Regression models are used to predict target variables on a continuous scale.\n",
        "\n",
        "Goal: model the relationship between one or multiple features and a continuous variable.\n",
        "*   Predict outputs on a continuous scale.\n",
        "\n",
        "Simple Linear Regression:\n",
        "*   Model the relationship between a single feature (explanatory variable, x) and a continuous target (response variable, y).\n",
        "*   y = w0 + w1x\n",
        "w0 = y axis intercept\n",
        "w1 = the weight coefficient of the explanatory variable\n",
        "*   Goal: learn the weights of the linear equation to describe the relationship between the explanatory variable and the target variable\n",
        "*   We are trying to find the best fitting line through the training examples.\n",
        "*   The vertical lines from the regression line to the training examples are the offsets or residuals (errors of the prediction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaBiosF71a8U"
      },
      "source": [
        "Multiple Linear Regression:\n",
        "y=W0X0 + W1X1 + ... + WmXm = Sum of WiXi = W^TX\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW8qg3sD2K4J"
      },
      "source": [
        "###Use EDA to look at the linear relationships###\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import scatterplotmatrix\n",
        "cols = ['col_1', 'col_2']\n",
        "\n",
        "scatterplotmatrix(df[cols].values, figsize=(10, 8),\n",
        "                  names=cols, alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfK2tK-59oEz"
      },
      "source": [
        "Use ordinary least squares (OLS) methhod to estimate the parameters of the linear regression line that minimizes the sum of the squared vertical distances (residuals or errors) to the training examples. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8Sb5xZK9m5l"
      },
      "source": [
        "X = df[['Feature']].values\n",
        "y = df['response variable'].values\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc_x = StandardScaler()\n",
        "\n",
        "sc_y = StandardScaler() \n",
        "\n",
        "X_std = sc_x.fit_transform(X)\n",
        "y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten() #back to a one-dimensional array\n",
        "lr = LinearRegressionGD()\n",
        "lr.fit(X_std, y_std)\n",
        "\n",
        "y_pred = lr.predict(X)\n",
        "\n",
        "print('Slope: %.3f' % slr.intercept_)\n",
        "\n",
        "###Visualize the linear regression line fit to training data###\n",
        "def lin_regplot(X, y, model):\n",
        "  plt.scatter(X, y, c='steelblue', edgecolor='white', s=70)\n",
        "  plt.plot(X, model.predict(X), color='black', lw=2)\n",
        "  return None\n",
        "\n",
        "lin_regplot(X, y, slr)\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "###Predicted values to back to original scale###\n",
        "\n",
        "variable = sc_x.transform(np.arrary([[numerical value of variable]]))\n",
        "y_variable_std = lr.predict(variable)\n",
        "print(\"_____\" % sc_y.inverse_transform(y_variable_std))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMj35_zZZoZp"
      },
      "source": [
        "###Evaluating the performance of linear regression models###\n",
        "##Estimate the generalization performance##\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df['y_variable'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "slr = LinearRegression()\n",
        "\n",
        "slr.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = slr.predict(X_train)\n",
        "##################\n",
        "X_test = X_test.reshape(1, -1)\n",
        "##################\n",
        "y_test_pred = slr.predict(X_test)\n",
        "\n",
        "plt.scatter(y_train_pred, y_train_pred - y_train,\n",
        "            c='steelblue', marker='o', edgecolor='white',\n",
        "            label='Training data')\n",
        "plt.scatter(y_test_pred, y_test_pred - y_test,\n",
        "            c='limegreen', marker='s', edgecolor='white',\n",
        "            label='Test data')\n",
        "\n",
        "plt.xlabel('Predicted values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.legend(loc='upper left')\n",
        "plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)\n",
        "plt.xlim([-10, 50])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}