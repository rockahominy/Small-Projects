{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNr02C7kNm/5BBC576G0OnG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rockahominy/Small-Projects/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utihl8KDgRdf"
      },
      "source": [
        "Perceptron will not converge if the classes are not perfectly linearly separable.\n",
        "\n",
        "**Logistic Regression:**\n",
        "*  Is a model for classification.\n",
        "*  Performs very well on linearly separable classes.\n",
        "*  Is also a linear model for classification, like the Perceptron and the Adaline models.\n",
        "\n",
        "**ODDS:**\n",
        "The odds in favor for a particular event can be written as p/(1 - p); where p is the probability of the positive event. \n",
        "\n",
        "The **logit function** is the logarithm of the odds:\n",
        "logit(p) = log(p/(1 - p))\n",
        "~The log refers to the natural logarithm\n",
        "*   The logit function takes input values from range 0 to 1 and transforms them to values over the entire real-number range.\n",
        "*   This can be used to express a linear relationship between feature values and the log-odds.\n",
        "*   logit(p(y=1|x)) = W0X0 + W1X1 + WmXm = Sum all these values up.\n",
        "*   The above equation represents the conditional probability that a particular example belongs to class 1 given its features, x.\n",
        "\n",
        "**Predicting the probability an example belongs in a particular class:**\n",
        "*   We will utilize the inverse form of the logit function to complete this task.\n",
        "*   The inverse form of the logit function is called the **logistic sigmoid function (sigmoid function).**\n",
        "*   Sigmoid function: O/(z) = (1)/1 + e^(-z)\n",
        "*   Here, z is the net input of the linear combination of weights and the inputs. z = wTx = w0x0 + w1x1 + wmxm\n",
        "*   w0 is the bias unit which is set to equal to 1.\n",
        "*   The outputs of the sigmoid function are constrained to 0 and 1.\n",
        "*   The sigmoid function is the activation function for logistic regression.\n",
        "*   The output of the sigmoid function is then interpreted as the probability of a particular example belonging to class 1, given its features x , parameterized by the weights w.\n",
        "*   If the predicted probability is >= 0.5, the example belongs to class 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4W52ry0o41i"
      },
      "source": [
        "**Logistic cost function--used to learn weights**\n",
        "Figuring out how to fit the parameters of the model:\n",
        "*   We need to maximize the likelihood L in L(w) = P(y|x;w)\n",
        "*   Apply the log function to maximize the likelihood function: l(w) = log L(w)\n",
        "*   Applying the log function reduces the potential for numerical underflow which can occur if the likelihoods are very small.\n",
        "*   Now use an optimization algortithm such as **gradient ascent to maximize the log-likelihood function.**\n",
        "*   A cost function J(w) can be used to minimize using gradient descent.\n",
        "\n",
        "**Loss functions for classification:**\n",
        "Binary cross-entropy is the loss function to train a binary classification (with a single output unit) model.\n",
        "**PAGE 540 Rachka**\n",
        "\n",
        "Loss functions availabel in Keras:\n",
        "*   Binary Cross entropy--Binary classification\n",
        "*   Categorical Cross entropy--multiclass classification\n",
        "*   Sparse categorical cross entropy--multiclass classification\n",
        "\n",
        "Computing the cross-entropy loss by providing the logits, and the not the class-membership probabilities is the preferred way. from_logits=True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXtSUnUy6FJV"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L72-gIiY6LlE",
        "outputId": "e1fd452b-eda2-4b91-f881-da64d62b504a"
      },
      "source": [
        "####Binary Crossentropy###\n",
        "\n",
        "bce_probas = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "bce_logits = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "logits = tf.constant([0.8])\n",
        "probas = tf.keras.activations.sigmoid(logits)\n",
        "\n",
        "tf.print('BCE (with probabilities: {:.4f}'.format(bce_probas(y_true=[1], y_pred=probas)),\n",
        "         'BCE (with Logits): {:.4f}'.format(bce_logits(y_true=[1], y_pred=logits))) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BCE (with probabilities: 0.3711 BCE (with Logits): 0.3711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7bgsEn-_x2o",
        "outputId": "ccd63b30-fb84-42be-9bf8-9b65afbcce46"
      },
      "source": [
        "from distutils.version import LooseVersion as Version\n",
        "####### Binary Crossentropy\n",
        "bce_probas = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "bce_logits = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "logits = tf.constant([0.8])\n",
        "probas = tf.keras.activations.sigmoid(logits)\n",
        "\n",
        "if Version(tf.__version__) >= '2.3.0':\n",
        "    tf.print(\n",
        "        'CCE (w Probas): {:.4f}'.format(bce_probas(y_true=[1], y_pred=probas)),\n",
        "        '(w Logits): {:.4f}'.format(bce_logits(y_true=[1], y_pred=logits)))\n",
        "    \n",
        "else:\n",
        "    tf.print(\n",
        "        'CCE (w Probas): {:.4f}'.format(\n",
        "        cce_probas(y_true=[0, 0, 1], y_pred=probas)),\n",
        "        '(w Logits): {:.4f}'.format(\n",
        "        cce_logits(y_true=[0, 0, 1], y_pred=logits)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CCE (w Probas): 0.3711 (w Logits): 0.3711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "nMbb0XAL_RwJ",
        "outputId": "747520c4-f0f0-46ff-fb7e-be47b6ce9e2a"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh6ZBSPK8FxG",
        "outputId": "d1378b85-6b28-46da-9be3-55c16b2631a0"
      },
      "source": [
        "###Categorical Crossentropy###\n",
        "\n",
        "####### Categorical Crossentropy\n",
        "cce_probas = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=False)\n",
        "cce_logits = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True)\n",
        "\n",
        "logits = tf.constant([[1.5, 0.8, 2.1]])\n",
        "probas = tf.keras.activations.softmax(logits)\n",
        "\n",
        "tf.print(\n",
        "    'CCE (w Probas): {:.4f}'.format(\n",
        "    cce_probas(y_true=[[0, 0, 1]], y_pred=probas)), #fixed it with double brackets\n",
        "    '(w Logits): {:.4f}'.format(\n",
        "    cce_logits(y_true=[[0, 0, 1]], y_pred=logits))) #fixed it with double brackets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CCE (w Probas): 0.5996 (w Logits): 0.5996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1xyQPAe_ks5",
        "outputId": "1c48682f-3a78-4f97-d10c-531cd2e056c5"
      },
      "source": [
        "####### Sparse Categorical Crossentropy\n",
        "####### Sparse Categorical Crossentropy\n",
        "sp_cce_probas = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False)\n",
        "sp_cce_logits = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True)\n",
        "\n",
        "tf.print(\n",
        "    'Sparse CCE (w Probas): {:.4f}'.format(\n",
        "    sp_cce_probas(y_true=[2], y_pred=probas)),\n",
        "    '(w Logits): {:.4f}'.format(\n",
        "    sp_cce_logits(y_true=[2], y_pred=logits)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sparse CCE (w Probas): 0.5996 (w Logits): 0.5996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRNAcoQtB73D"
      },
      "source": [
        "**Gradient Tape:**\n",
        "\n",
        "Optimizing neural networks requires computing the gradients of the cost with respest to the NN weights. This is required for optimization algorithms such as stochastic gradient descent (SGD).\n",
        "\n",
        "GradientTape is used to compute gradients.\n",
        "\n",
        "In order to compute the gradients of the tensors, we have to record the computations via:\n",
        "\n",
        "**tf.GradientTape**\n",
        "GradientTape allows for automatic differentiation.\n",
        "**Rachka Page 483**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkoqB1nCEvaA",
        "outputId": "aa248fb0-9399-4bb0-b47d-d106481a791f"
      },
      "source": [
        "#Compute z = wx + b\n",
        "#Define the loss the squared loss between the target and prediction:\n",
        "#Loss = (y - z)^2\n",
        "\n",
        "#Defining the model parameters w and b\n",
        "w = tf.Variable(1.0)\n",
        "b = tf.Variable(0.5)\n",
        "print(w.trainable, b.trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHK9uBKYF0Xs",
        "outputId": "f76c39e6-a583-42b6-991d-7b5485c28f8c"
      },
      "source": [
        "#Defining the input x and y as tensors\n",
        "x = tf.convert_to_tensor([1.4])\n",
        "y = tf.convert_to_tensor([2.1])\n",
        "with tf.GradientTape() as tape: #tape object where the operations will be recorded\n",
        "  z = tf.add(tf.multiply(w, x), b)\n",
        "  loss = tf.reduce_sum(tf.square(y - z))\n",
        "\n",
        "dloss_dw = tape.gradient(loss, w)\n",
        "tf.print('dL/dw:', dloss_dw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dL/dw: -0.559999764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuFhF5tfHMAi",
        "outputId": "7d3c6a41-0075-475a-8be5-839dcf39f486"
      },
      "source": [
        "#verifying the computed gradient\n",
        "tf.print(2*x*(w*x+b-y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.559999764]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMh8bVRIIRWM"
      },
      "source": [
        "The universal approximation theorem states that a feedforward NN with a single hidden layer and a relatively large number of hidden units can approximate arbitrary continuous functions relatively well. \n",
        "**Rachska Page 493**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK7sj8s8JvQV"
      },
      "source": [
        "MIT Deep Reinforcement Learning\n",
        "\n",
        "In **supervised learning** there's a dataset and our goal is to learn and learn a model to predict the label y. \n",
        "\n",
        "In **unsupervised learning** there's only data with no labels and our goal is to find underlying structure in the data. \n",
        "\n",
        "In **reinforcement learning** the data is given as state-action pairs. Our goal is to maximize the future rewards over many time steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPRT6fTfK9pw"
      },
      "source": [
        "**Agent:**\n",
        "*   Central part to the reainforcement learning algorithm.\n",
        "*   Is the nueral network.\n",
        "*   Example: a drone is the agent.\n",
        "\n",
        "**Environment:**\n",
        "*   The environment which the agent acts.\n",
        "\n",
        "The agent sends commands to the environment in the form of actions. \n",
        "Action space A: the set of possible actions an agent can make in the environment.\n",
        "In return, the environment will send back observations back to the agent. The environment sends back observations as a **state**. A state is a situation which the agent perceives.\n",
        "\n",
        "The goal of reinforcement learning is for the agent to maximize its rewards in this environment. For every step, the agent is also getting back a reward from the environment. The **reward** is just a feedback that measures the success or failure of the agent's action.\n",
        "\n",
        "Total Reward is the summation of the rewards at some time. \n",
        "\n",
        "The **discounted reward **  is obtained by multiplying the discounting factor lambda by each of the rewards. You do this is discount future rewards so they don't count as much as the current reward. The discounting factor is typically between 0 and 1. \n",
        "\n",
        "**Q function:**\n",
        "Q(st, at) = E[Rt|st, at]\n",
        "The Q function captures the expected total future reward an agent in state, s, can receive by executing a certain action, a.\n",
        "High Q value = we're taking an action that's desirable at that state.\n",
        "Low Q value = we're taking an action that's undesirable at that state.\n",
        "\n",
        "Ultimately the agent needs a **policy pi(s)** to infer the best action to take at its state, s. \n",
        "\n",
        "Strategy: the policy should choose an action that maximizes future reward.\n",
        "\n",
        "Evaluate the Q function over all possible actions and pick the action the Q function is our policy. \n",
        "\n",
        "-------Value Learning-------\n",
        "Deep Q Networks (DQN):\n",
        "Action + State --> Expected Return\n",
        "*This is time extensive because each action has to the calculated separately. \n",
        "\n",
        "Alternative:\n",
        "State --> Expected Return for each action\n",
        "*Only needs to be executed once and we get the Q values for every single action.\n",
        "*We look at the Q values and pick the maximum and take that action.\n",
        "\n",
        "Q-Loss function:\n",
        "Target Q value - Predicted Q value\n",
        "*Loss function is the mean squared error between the taget Q value and predicted Q value from the network.\n",
        "*Take the argmax of the Q values in a state to figure out the action.\n",
        "\n",
        "Caveats of Q-Learning:\n",
        "*Doesn't do well with complex model scenarios. Q-learning is well suited where the action space is discrete and small. It cannot handel continuous action spaces. \n",
        "*The policy is deterministic because we are picking the action that maximizes the Q value. By maximizing the reward, we cannot learn from stochastic policies.\n",
        "\n",
        "-------Policy Learning-------\n",
        "Instead of calculating the Q function and then determine the policy, we are going to directly take the state and action and compute the policy of the network.\n",
        "So, output a probability distribution over the space of all actions given that state. The probability distribution is the policy and we can take an action based on the policy. Just sample from the probability distribution and act accordingly to the action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUJTQ1WOc3Q6"
      },
      "source": [
        "**Limitations of Deep Learning:**\n",
        "\n",
        "Universal Approximation Theorem:\n",
        "*   A feedforward network with a single layer is sufficient to approximate, to an arbitrary precision, any continuous function.\n",
        "*   Caveats:\n",
        "  *This theorem makes no guarantees about the number of hidden units or the size of the hidden layer that would be required to make the approximation. And, it makes no suggestions on how to find the weights and optimize the network for the task. \n",
        "\n",
        "Neural Network Limitations:\n",
        "-Very data hungry\n",
        "-Computationally intensive (GPUs needed)\n",
        "-Easily fooled by adversarial examples\n",
        "-Can be subject to algorithmic bias\n",
        "-Difficult\n",
        " to encode structure and prior knowledge during learning\n",
        "-Poor at representing uncertainty (how do you know what the model knows?)\n",
        "-Uninterpretable black boxes, difficult to trust\n",
        "-Finicky to optimize\n",
        "-Often requires expert knowledge to design, fine tune architectures.\n",
        "\n",
        "\n"
      ]
    }
  ]
}